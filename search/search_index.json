{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pyef","text":"<p>Energy Forecasting Toolkit in Python This is a work in progress. Do not use</p>"},{"location":"#installation","title":"Installation","text":"<p>With <code>pip</code>: <pre><code>pip install pyef\n</code></pre></p> <p>With <code>pipx</code>: <pre><code>python3.7 -m pip install --user pipx\npipx install pyef\n</code></pre></p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unrealeased","title":"Unrealeased","text":"<ul> <li>TimeFrames - Added support for infering frequency</li> <li>Evaluator - Updated the ModelGridSearch to support better logging and parallel execution</li> <li>Datasets - Removed csv files and add zipped data files instead</li> <li>General - Added ruff support, slow marker for long tests and more</li> </ul> <p>Compare with latest</p>"},{"location":"changelog/#010-2022-12-23","title":"0.1.0 - 2022-12-23","text":"<ul> <li>TimeFrames - Contains EnergyTimeFrame class which preprocesses energy time series data to make it sutible to feed to Forecaster.</li> <li>Forecaster - Forecaster class takes EnergyTimeFrame and sklearn compatible model to get a forecast.</li> <li>Evaluator - Evaluator class will contain different metrics that can be calculated from the forecasted values.</li> <li>Datasets - Some commonly used Energy datasets.</li> </ul> <p>Compare with first commit</p>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at sardana.bhav@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#environment-setup","title":"Environment setup","text":"<p>Nothing easier!</p> <p>Fork and clone the repository, then:</p> <pre><code>cd pyef\nmake setup\n</code></pre> <p>Note</p> <p> If it fails for some reason, you'll need to install PDM manually.</p> <p>You can install it with:</p> <pre><code>python3 -m pip install --user pipx\npipx install pdm\n</code></pre> <p>Now you can try running <code>make setup</code> again, or simply <code>pdm install</code>.</p> <p>You now have the dependencies installed.</p> <p>You can run the application with <code>pdm run pyef [ARGS...]</code>.</p> <p>Run <code>make help</code> to see all the available actions!</p>"},{"location":"contributing/#tasks","title":"Tasks","text":"<p>This project uses duty to run tasks. A Makefile is also provided. The Makefile will try to run certain tasks on multiple Python versions. If for some reason you don't want to run the task on multiple Python versions, you can do one of the following:</p> <ol> <li><code>export PYTHON_VERSIONS=</code>: this will run the task    with only the current Python version</li> <li>run the task directly with <code>pdm run duty TASK</code></li> </ol> <p>The Makefile detects if a virtual environment is activated, so <code>make</code> will work the same with the virtualenv activated or not.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>As usual:</p> <ol> <li>create a new branch: <code>git checkout -b feature-or-bugfix-name</code></li> <li>edit the code and/or the documentation</li> </ol> <p>Before committing:</p> <ol> <li>run <code>make format</code> to auto-format the code</li> <li>run <code>make check</code> to check everything (fix any warning)</li> <li>run <code>make test</code> to run the tests (fix any issue)</li> <li>if you updated the documentation or the project dependencies:<ol> <li>run <code>make docs-serve</code></li> <li>go to http://localhost:8000 and check that everything looks good</li> </ol> </li> <li>follow our commit message convention</li> </ol> <p>If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review.</p> <p>Don't bother updating the changelog, we will take care of this.</p>"},{"location":"contributing/#commit-message-convention","title":"Commit message convention","text":"<p>Commits messages must follow the Angular style:</p> <pre><code>&lt;type&gt;[(scope)]: Subject\n\n[Body]\n</code></pre> <p>Scope and body are optional. Type can be:</p> <ul> <li><code>build</code>: About packaging, building wheels, etc.</li> <li><code>chore</code>: About packaging or repo/files management.</li> <li><code>ci</code>: About Continuous Integration.</li> <li><code>docs</code>: About documentation.</li> <li><code>feat</code>: New feature.</li> <li><code>fix</code>: Bug fix.</li> <li><code>perf</code>: About performance.</li> <li><code>refactor</code>: Changes which are not features nor bug fixes.</li> <li><code>style</code>: A change in code style/format.</li> <li><code>tests</code>: About tests.</li> </ul> <p>Subject (and body) must be valid Markdown. If you write a body, please add issues references at the end:</p> <pre><code>Body.\n\nReferences: #10, #11.\nFixes #15.\n</code></pre>"},{"location":"contributing/#pull-requests-guidelines","title":"Pull requests guidelines","text":"<p>Link to any related issue in the Pull Request message.</p> <p>During review, we recommend using fixups:</p> <pre><code># SHA is the SHA of the commit you want to fix\ngit commit --fixup=SHA\n</code></pre> <p>Once all the changes are approved, you can squash your commits:</p> <pre><code>git rebase -i --autosquash master\n</code></pre> <p>And force-push:</p> <pre><code>git push -f\n</code></pre> <p>If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.</p>"},{"location":"credits/","title":"Credits","text":"<p>These projects were used to build <code>pyef</code>. Thank you!</p> <p><code>python</code> | <code>pdm</code> | <code>copier-pdm</code></p>"},{"location":"credits/#exec-0--runtime-dependencies","title":"Runtime dependencies","text":"Project Summary Version (accepted) Version (last resolved) License <code>contourpy</code> Python library for calculating contours of 2D quadrilateral grids <code>&gt;=1.0.1</code> <code>1.0.6</code> BSD-3-Clause <code>cycler</code> Composable style cycles <code>&gt;=0.10</code> <code>0.11.0</code> BSD <code>fonttools</code> Tools to manipulate font files <code>&gt;=4.22.0</code> <code>4.38.0</code> MIT <code>importlib-resources</code> Read resources from Python packages <code>&gt;=5.12.0</code> <code>5.12.0</code> Apache Software License <code>joblib</code> Lightweight pipelining with Python functions <code>&gt;=1.0.1</code> <code>1.2.0</code> BSD <code>kiwisolver</code> A fast implementation of the Cassowary constraint solver <code>&gt;=1.0.1</code> <code>1.4.4</code> BSD License <code>markdown-it-py</code> Python port of markdown-it. Markdown parsing, done right! <code>&lt;3.0.0,&gt;=2.2.0</code> <code>2.2.0</code> MIT License <code>matplotlib</code> Python plotting package <code>&gt;=3.6.2</code> <code>3.6.2</code> PSF <code>mdurl</code> Markdown URL utilities <code>~=0.1</code> <code>0.1.2</code> MIT License <code>memo</code> UNKNOWN <code>&gt;=0.2.3</code> <code>0.2.3</code> UNKNOWN <code>numpy</code> Fundamental package for array computing in Python <code>==1.24.0</code> <code>1.24.0</code> BSD-3-Clause <code>optioneer</code> Nestable, tab-able options for use in Python programs <code>&gt;=1.0.3</code> <code>1.0.3</code> BSD license <code>orjson</code> Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy <code>&gt;=3.4.5</code> <code>3.8.3</code> Apache-2.0 OR MIT <code>packaging</code> Core utilities for Python packages <code>&gt;=20.0</code> <code>23.0</code> BSD License <code>pandas</code> Powerful data structures for data analysis, time series, and statistics <code>&gt;=1.5.2</code> <code>1.5.3</code> BSD-3-Clause <code>patsy</code> A Python package for describing statistical models and for building design matrices. <code>&gt;=0.5.3</code> <code>0.5.3</code> 2-clause BSD <code>pillow</code> Python Imaging Library (Fork) <code>&gt;=6.2.0</code> <code>9.3.0</code> HPND <code>plotly</code> An open-source, interactive data visualization library for Python <code>&gt;=5.11.0</code> <code>5.13.1</code> MIT <code>pygments</code> Pygments is a syntax highlighting package written in Python. <code>&lt;3.0.0,&gt;=2.13.0</code> <code>2.14.0</code> BSD-2-Clause <code>pyparsing</code> pyparsing module - Classes and methods to define and execute parsing grammars <code>&gt;=2.2.1</code> <code>3.0.9</code> MIT License <code>python-dateutil</code> Extensions to the standard Python datetime module <code>&gt;=2.7</code> <code>2.8.2</code> Dual License <code>pytz</code> World timezone definitions, modern and historical <code>&gt;=2020.1</code> <code>2022.7</code> MIT <code>rich</code> Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal <code>&gt;=12.6.0</code> <code>13.3.2</code> MIT <code>scikit-learn</code> A set of python modules for machine learning and data mining <code>&gt;=1.2.0</code> <code>1.2.2</code> new BSD <code>scipy</code> Fundamental algorithms for scientific computing in Python <code>&gt;=1.3.2</code> <code>1.9.3</code> BSD License <code>six</code> Python 2 and 3 compatibility utilities <code>1.16.0</code> MIT <code>tenacity</code> Retry code until it succeeds <code>&gt;=6.2.0</code> <code>8.1.0</code> Apache 2.0 <code>threadpoolctl</code> threadpoolctl <code>&gt;=2.0.0</code> <code>3.1.0</code> BSD-3-Clause <code>typing-extensions</code> Backported and Experimental Type Hints for Python 3.7+ <code>&lt;5.0,&gt;=4.0.0; python_version &lt; \"3.9\"</code> <code>4.4.0</code> Python Software Foundation License <code>zipp</code> Backport of pathlib-compatible object wrapper for zip files <code>&gt;=3.1.0; python_version &lt; \"3.10\"</code> <code>3.11.0</code> MIT License"},{"location":"credits/#exec-0--development-dependencies","title":"Development dependencies","text":"Project Summary Version (accepted) Version (last resolved) License <code>ansimarkup</code> Produce colored terminal text with an xml-like markup <code>~=1.4</code> <code>1.5.0</code> Revised BSD License <code>anyio</code> High level compatibility layer for multiple asynchronous event loop implementations <code>&lt;4,&gt;=3.1.0</code> <code>3.6.2</code> MIT <code>appnope</code> Disable App Nap on macOS &gt;= 10.9 <code>; platform_system == \"Darwin\"</code> <code>0.1.3</code> BSD <code>argon2-cffi</code> The secure Argon2 password hashing algorithm. <code>21.3.0</code> MIT License <code>argon2-cffi-bindings</code> Low-level CFFI bindings for Argon2 <code>21.2.0</code> MIT <code>arrow</code> Better dates &amp; times for Python <code>&gt;=0.15.0</code> <code>1.2.3</code> Apache 2.0 <code>astor</code> Read/rewrite/write Python ASTs <code>&gt;=0.8</code> <code>0.8.1</code> BSD-3-Clause <code>asttokens</code> Annotate AST trees with source code positions <code>&gt;=2.1.0</code> <code>2.2.1</code> Apache 2.0 <code>attrs</code> Classes Without Boilerplate <code>&gt;=19.2.0</code> <code>22.2.0</code> MIT <code>autoflake</code> Removes unused imports and unused variables <code>&gt;=1.4</code> <code>1.7.8</code> MIT <code>backcall</code> Specifications for callback functions passed in to an API <code>0.2.0</code> BSD License <code>bandit</code> Security oriented static analyser for python code. <code>&gt;=1.7.3</code> <code>1.7.4</code> Apache-2.0 license <code>beautifulsoup4</code> Screen-scraping library <code>4.11.1</code> MIT <code>black</code> The uncompromising code formatter. <code>&gt;=21.10b0</code> <code>23.1.0</code> MIT <code>bleach</code> An easy safelist-based HTML-sanitizing tool. <code>5.0.1</code> Apache Software License <code>certifi</code> Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2022.12.7</code> MPL-2.0 <code>cffi</code> Foreign Function Interface for Python calling C code. <code>; implementation_name == \"pypy\"</code> <code>1.15.1</code> MIT <code>charset-normalizer</code> The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&lt;3,&gt;=2</code> <code>2.1.1</code> MIT <code>click</code> Composable command line interface toolkit <code>&gt;=8.0.0</code> <code>8.1.3</code> BSD-3-Clause <code>colorama</code> Cross-platform colored terminal text. <code>; platform_system == \"Windows\"</code> <code>0.4.6</code> BSD License <code>comm</code> Jupyter Python Comm implementation, for usage in ipykernel, xeus-python etc. <code>&gt;=0.1.1</code> <code>0.1.2</code> BSD License <code>coverage</code> Code coverage measurement for Python <code>[toml]&gt;=5.2.1</code> <code>7.0.0</code> Apache-2.0 <code>cryptography</code> cryptography is a package which provides cryptographic recipes and primitives to Python developers. <code>&gt;=2.0</code> <code>38.0.4</code> ? <code>darglint</code> A utility for ensuring Google-style docstrings stay up to date with the source code. <code>&gt;=1.8</code> <code>1.8.1</code> MIT <code>debugpy</code> An implementation of the Debug Adapter Protocol for Python <code>&gt;=1.6.5</code> <code>1.6.6</code> MIT <code>decorator</code> Decorators for Humans <code>5.1.1</code> new BSD License <code>defusedxml</code> XML bomb protection for Python stdlib modules <code>0.7.1</code> PSFL <code>docutils</code> Docutils -- Python Documentation Utilities <code>&gt;=0.13.1</code> <code>0.19</code> public domain, Python, 2-Clause BSD, GPL 3 (see COPYING.txt) <code>dparse</code> A parser for Python dependency files <code>&gt;=0.6.2</code> <code>0.6.2</code> MIT license <code>duty</code> A simple task runner. <code>&gt;=0.7</code> <code>0.9.0</code> ISC <code>editables</code> Editable installations <code>&gt;=0.3</code> <code>0.3</code> MIT <code>entrypoints</code> Discover and load entry points from installed packages. <code>0.4</code> MIT License <code>exceptiongroup</code> Backport of PEP 654 (exception groups) <code>&gt;=1.0.0rc8; python_version &lt; \"3.11\"</code> <code>1.0.4</code> ? <code>execnet</code> execnet: rapid multi-Python deployment <code>&gt;=1.1</code> <code>1.9.0</code> MIT <code>executing</code> Get the currently executing AST node of a frame, and other information <code>&gt;=1.2.0</code> <code>1.2.0</code> MIT <code>failprint</code> Run a command, print its output only if it fails. <code>&gt;=0.10</code> <code>0.10.0</code> ISC <code>fastjsonschema</code> Fastest Python implementation of JSON schema <code>2.16.2</code> BSD <code>flake8</code> the modular source code checker: pep8 pyflakes and co <code>&gt;=4; python_version &gt;= '3.8'</code> <code>5.0.4</code> MIT <code>flake8-bandit</code> Automated security testing with bandit and flake8. <code>&gt;=2.1</code> <code>4.1.1</code> MIT <code>flake8-black</code> flake8 plugin to call black as a code style validator <code>&gt;=0.2</code> <code>0.3.6</code> MIT <code>flake8-bugbear</code> A plugin for flake8 finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. <code>&gt;=21.9</code> <code>23.2.13</code> MIT <code>flake8-builtins</code> Check for python builtins being used as variables or parameters. <code>&gt;=1.5</code> <code>2.1.0</code> GPL version 2 <code>flake8-comprehensions</code> A flake8 plugin to help you write better list/set/dict comprehensions. <code>&gt;=3.7</code> <code>3.10.1</code> MIT <code>flake8-docstrings</code> Extension for flake8 which uses pydocstyle to check docstrings <code>&gt;=1.6</code> <code>1.7.0</code> MIT <code>flake8-plugin-utils</code> The package provides base classes and utils for flake8 plugin writing <code>&lt;2.0.0,&gt;=1.3.2</code> <code>1.3.2</code> MIT <code>flake8-polyfill</code> Polyfill package for Flake8 plugins <code>&gt;=1.0.2</code> <code>1.0.2</code> MIT <code>flake8-pytest-style</code> A flake8 plugin checking common style issues or inconsistencies with pytest-based tests. <code>&gt;=1.5</code> <code>1.7.2</code> MIT <code>flake8-string-format</code> string format checker, plugin for flake8 <code>&gt;=0.3</code> <code>0.3.0</code> MIT License <code>flake8-tidy-imports</code> A flake8 plugin that helps you write tidier imports. <code>&gt;=4.5</code> <code>4.8.0</code> MIT <code>flake8-variables-names</code> A flake8 extension that helps to make more readable variables names <code>&gt;=0.0</code> <code>0.0.5</code> MIT <code>fqdn</code> Validates fully-qualified domain names against RFC 1123, so that they are acceptable to modern bowsers <code>1.5.1</code> MPL 2.0 <code>ghp-import</code> Copy your docs directly to the gh-pages branch. <code>&gt;=1.0</code> <code>2.1.0</code> Apache Software License <code>git-changelog</code> Automatic Changelog generator using Jinja2 templates. <code>&gt;=0.4</code> <code>1.0.0</code> ISC <code>gitdb</code> Git Object Database <code>&lt;5,&gt;=4.0.1</code> <code>4.0.10</code> BSD License <code>gitpython</code> GitPython is a python library used to interact with Git repositories <code>&gt;=1.0.1</code> <code>3.1.29</code> BSD <code>griffe</code> Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. <code>&gt;=0.24</code> <code>0.25.1</code> ISC <code>idna</code> Internationalized Domain Names in Applications (IDNA) <code>&lt;4,&gt;=2.5</code> <code>3.4</code> BSD License <code>importlib-metadata</code> Read metadata from Python packages <code>&gt;=4.3; python_version &lt; \"3.10\"</code> <code>4.13.0</code> Apache Software License <code>importlib-resources</code> Read resources from Python packages <code>; python_version &lt; \"3.9\"</code> <code>5.12.0</code> Apache Software License <code>iniconfig</code> iniconfig: brain-dead simple config-ini parsing <code>1.1.1</code> MIT License <code>ipykernel</code> IPython Kernel for Jupyter <code>&gt;=6.19.4</code> <code>6.21.3</code> BSD License <code>ipython</code> IPython: Productive Interactive Computing <code>&gt;=7.23.1</code> <code>8.7.0</code> BSD-3-Clause <code>ipython-genutils</code> Vestigial utilities from IPython <code>0.2.0</code> BSD <code>ipywidgets</code> Jupyter interactive widgets <code>8.0.4</code> BSD 3-Clause License <code>isoduration</code> Operations with ISO 8601 durations <code>20.11.0</code> ISC License (ISCL) <code>isort</code> A Python utility / library to sort Python imports. <code>&gt;=5.10</code> <code>5.12.0</code> MIT <code>jaraco-classes</code> Utility functions for Python class constructs <code>3.2.3</code> MIT License <code>jedi</code> An autocompletion tool for Python that can be used for text editors. <code>&gt;=0.16</code> <code>0.18.2</code> MIT <code>jeepney</code> Low-level, pure Python DBus protocol wrapper. <code>&gt;=0.4.2; sys_platform == \"linux\"</code> <code>0.8.0</code> ? <code>jinja2</code> A very fast and expressive template engine. <code>&lt;4,&gt;=2.11</code> <code>3.1.2</code> BSD-3-Clause <code>jsonpointer</code> Identify specific nodes in a JSON document (RFC 6901) <code>&gt;1.13</code> <code>2.3</code> Modified BSD License <code>jsonschema</code> An implementation of JSON Schema validation for Python <code>&gt;=2.6</code> <code>4.17.3</code> MIT <code>jupyter</code> Jupyter metapackage. Install all the Jupyter components in one go. <code>&gt;=1.0.0</code> <code>1.0.0</code> BSD <code>jupyter-client</code> Jupyter protocol implementation and client libraries <code>&gt;=6.1.12</code> <code>7.4.8</code> BSD License <code>jupyter-console</code> Jupyter terminal console <code>6.4.4</code> BSD <code>jupyter-core</code> Jupyter core package. A base package on which Jupyter projects rely. <code>!=5.0.*,&gt;=4.12</code> <code>5.1.1</code> BSD License <code>jupyter-events</code> Jupyter Event System library <code>&gt;=0.4.0</code> <code>0.5.0</code> BSD License <code>jupyter-server</code> The backend\u2014i.e. core services, APIs, and REST endpoints\u2014to Jupyter web applications. <code>&gt;=1.8</code> <code>2.0.5</code> BSD License <code>jupyter-server-terminals</code> A Jupyter Server Extension Providing Terminals. <code>0.4.3</code> BSD License <code>jupyterlab-pygments</code> Pygments theme using JupyterLab CSS variables <code>0.2.2</code> BSD <code>jupyterlab-widgets</code> Jupyter interactive widgets for JupyterLab <code>~=3.0</code> <code>3.0.5</code> BSD-3-Clause <code>keyring</code> Store and access your passwords safely. <code>&gt;=15.1</code> <code>23.13.1</code> MIT License <code>markdown</code> Python implementation of Markdown. <code>&lt;4.0.0,&gt;=3.3.3</code> <code>3.3.7</code> BSD License <code>markdown-callouts</code> Markdown extension: a classier syntax for admonitions <code>&gt;=0.2</code> <code>0.3.0</code> MIT <code>markdown-exec</code> Utilities to execute code blocks in Markdown files. <code>&gt;=0.5</code> <code>1.3.0</code> ISC <code>markdown-it-py</code> Python port of markdown-it. Markdown parsing, done right! <code>&lt;3.0.0,&gt;=2.2.0</code> <code>2.2.0</code> MIT License <code>markupsafe</code> Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0</code> <code>2.1.1</code> BSD-3-Clause <code>matplotlib-inline</code> Inline Matplotlib backend for Jupyter <code>&gt;=0.1</code> <code>0.1.6</code> BSD 3-Clause <code>mccabe</code> McCabe checker, plugin for flake8 <code>&lt;0.8.0,&gt;=0.7.0</code> <code>0.7.0</code> Expat license <code>mdurl</code> Markdown URL utilities <code>~=0.1</code> <code>0.1.2</code> MIT License <code>mergedeep</code> A deep merge function for \ud83d\udc0d. <code>&gt;=1.3.4</code> <code>1.3.4</code> MIT License <code>mistune</code> A sane Markdown parser with useful plugins and renderers <code>&lt;3,&gt;=2.0.3</code> <code>2.0.4</code> BSD 3-Clause License <code>mkdocs</code> Project documentation with Markdown. <code>&gt;=1.3</code> <code>1.4.2</code> BSD License <code>mkdocs-coverage</code> MkDocs plugin to integrate your coverage HTML report into your site. <code>&gt;=0.2</code> <code>0.2.6</code> ISC <code>mkdocs-gen-files</code> MkDocs plugin to programmatically generate documentation pages during the build <code>&gt;=0.3</code> <code>0.4.0</code> MIT <code>mkdocs-literate-nav</code> MkDocs plugin to specify the navigation in Markdown instead of YAML <code>&gt;=0.4</code> <code>0.6.0</code> MIT License <code>mkdocs-material</code> Documentation that simply works <code>&gt;=7.3</code> <code>9.1.2</code> MIT License <code>mkdocs-material-extensions</code> Extension pack for Python Markdown and MkDocs Material. <code>&gt;=1.1</code> <code>1.1.1</code> MIT License <code>mkdocs-section-index</code> MkDocs plugin to allow clickable sections that lead to an index page <code>&gt;=0.3</code> <code>0.3.5</code> MIT License <code>mkdocstrings</code> Automatic documentation from sources, for MkDocs. <code>[python]&gt;=0.18</code> <code>0.20.0</code> ISC <code>mkdocstrings-python</code> A Python handler for mkdocstrings. <code>&gt;=0.5.2</code> <code>0.8.2</code> ISC <code>more-itertools</code> More routines for operating on iterables, beyond itertools <code>9.0.0</code> MIT License <code>mypy</code> Optional static typing for Python <code>&gt;=0.910</code> <code>1.1.1</code> MIT License <code>mypy-extensions</code> Type system extensions for programs checked with the mypy type checker. <code>&gt;=0.4.3</code> <code>1.0.0</code> MIT License <code>nbclassic</code> A web-based notebook environment for interactive computing <code>&gt;=0.4.7</code> <code>0.4.8</code> BSD <code>nbclient</code> A client library for executing notebooks. Formerly nbconvert's ExecutePreprocessor. <code>&gt;=0.5.0</code> <code>0.7.2</code> BSD License <code>nbconvert</code> Converting Jupyter Notebooks <code>7.2.7</code> BSD License <code>nbformat</code> The Jupyter Notebook format <code>&gt;=5.1</code> <code>5.7.1</code> BSD License <code>nest-asyncio</code> Patch asyncio to allow nested event loops <code>1.5.6</code> BSD <code>notebook</code> A web-based notebook environment for interactive computing <code>&gt;=6.5.3</code> <code>6.5.3</code> BSD <code>notebook-shim</code> A shim layer for notebook traits and config <code>&gt;=0.1.0</code> <code>0.2.2</code> BSD License <code>packaging</code> Core utilities for Python packages <code>&gt;=22.0</code> <code>23.0</code> BSD License <code>pandocfilters</code> Utilities for writing pandoc filters in python <code>&gt;=1.4.1</code> <code>1.5.0</code> BSD-3-Clause <code>parso</code> A Python Parser <code>&lt;0.9.0,&gt;=0.8.0</code> <code>0.8.3</code> MIT <code>pathspec</code> Utility library for gitignore style pattern matching of file paths. <code>&gt;=0.9.0</code> <code>0.10.3</code> MPL 2.0 <code>pbr</code> Python Build Reasonableness <code>!=2.1.0,&gt;=2.0.0</code> <code>5.11.0</code> Apache Software License <code>pep8-naming</code> Check PEP-8 naming conventions, plugin for flake8 <code>&gt;=0.12</code> <code>0.13.3</code> Expat license <code>pexpect</code> Pexpect allows easy control of interactive console applications. <code>&gt;4.3; sys_platform != \"win32\"</code> <code>4.8.0</code> ISC license <code>pickleshare</code> Tiny 'shelve'-like database with concurrency support <code>0.7.5</code> MIT <code>pkginfo</code> Query metadatdata from sdists / bdists / installed packages. <code>&gt;=1.8.1</code> <code>1.9.2</code> MIT <code>platformdirs</code> A small Python package for determining appropriate platform-specific dirs, e.g. a \"user data dir\". <code>&gt;=2</code> <code>2.6.0</code> MIT License <code>pluggy</code> plugin and hook calling mechanisms for python <code>&lt;2.0,&gt;=0.12</code> <code>1.0.0</code> MIT <code>prometheus-client</code> Python client for the Prometheus monitoring system. <code>0.15.0</code> Apache Software License 2.0 <code>prompt-toolkit</code> Library for building powerful interactive command lines in Python <code>&lt;3.1.0,&gt;=3.0.11</code> <code>3.0.36</code> BSD License <code>psutil</code> Cross-platform lib for process and system monitoring in Python. <code>5.9.4</code> BSD-3-Clause <code>ptyprocess</code> Run a subprocess in a pseudo terminal <code>~=0.6; sys_platform != \"win32\"</code> <code>0.7.0</code> ISC License (ISCL) <code>pure-eval</code> Safely evaluate AST nodes without side effects <code>0.2.2</code> MIT <code>pycodestyle</code> Python style guide checker <code>&lt;2.10.0,&gt;=2.9.0</code> <code>2.9.1</code> Expat license <code>pycparser</code> C parser in Python <code>2.21</code> BSD <code>pydocstyle</code> Python docstring style checker <code>&gt;=2.1</code> <code>6.1.1</code> MIT <code>pyflakes</code> passive checker of Python programs <code>&lt;3,&gt;=1.1.0</code> <code>2.5.0</code> MIT <code>pygments</code> Pygments is a syntax highlighting package written in Python. <code>&gt;=2.4.0</code> <code>2.14.0</code> BSD-2-Clause <code>pyinstrument</code> Call stack profiler for Python. Shows you why your code is slow! <code>&gt;=4.4.0</code> <code>4.4.0</code> BSD License <code>pymdown-extensions</code> Extension pack for Python Markdown. <code>&gt;=9</code> <code>9.10</code> MIT License <code>pytest</code> pytest: simple powerful testing with Python <code>&gt;=6.2</code> <code>7.2.2</code> MIT <code>pytest-cov</code> Pytest plugin for measuring coverage. <code>&gt;=3.0</code> <code>4.0.0</code> MIT <code>pytest-randomly</code> Pytest plugin to randomly order tests and control random.seed. <code>&gt;=3.10</code> <code>3.12.0</code> MIT <code>pytest-xdist</code> pytest xdist plugin for distributed testing, most importantly across multiple CPUs <code>&gt;=2.4</code> <code>3.2.0</code> MIT <code>python-dateutil</code> Extensions to the standard Python datetime module <code>&gt;=2.8.2</code> <code>2.8.2</code> Dual License <code>python-json-logger</code> A python library adding a json log formatter <code>2.0.4</code> BSD <code>pywin32</code> Python for Window Extensions <code>&gt;=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\"</code> <code>305</code> ? <code>pywin32-ctypes</code> UNKNOWN <code>&gt;=0.2.0; sys_platform == \"win32\"</code> <code>0.2.0</code> ? <code>pywinpty</code> Pseudo terminal support for Windows from Python. <code>&gt;=1.1.0; os_name == \"nt\"</code> <code>2.0.9</code> ? <code>pyyaml</code> YAML parser and emitter for Python <code>&gt;=5.1</code> <code>6.0</code> MIT <code>pyyaml-env-tag</code> A custom YAML tag for referencing environment variables in YAML files. <code>&gt;=0.1</code> <code>0.1</code> MIT License <code>pyzmq</code> Python bindings for 0MQ <code>&gt;=25.0.0</code> <code>25.0.0</code> LGPL+BSD <code>qtconsole</code> Jupyter Qt console <code>5.4.0</code> BSD <code>qtpy</code> Provides an abstraction layer on top of the various Qt bindings (PyQt5/6 and PySide2/6). <code>&gt;=2.0.1</code> <code>2.3.0</code> MIT <code>readme-renderer</code> readme_renderer is a library for rendering \"readme\" descriptions for Warehouse <code>&gt;=35.0</code> <code>37.3</code> Apache License, Version 2.0 <code>regex</code> Alternative regular expression module, to replace re. <code>&gt;=2022.4.24</code> <code>2022.10.31</code> Apache Software License <code>requests</code> Python HTTP for Humans. <code>&gt;=2.26</code> <code>2.28.1</code> Apache 2.0 <code>requests-toolbelt</code> A utility belt for advanced users of python-requests <code>!=0.9.0,&gt;=0.8.0</code> <code>0.10.1</code> Apache 2.0 <code>rfc3339-validator</code> A pure python RFC3339 validator <code>0.1.4</code> MIT license <code>rfc3986</code> Validating URI References per RFC 3986 <code>&gt;=1.4.0</code> <code>2.0.0</code> Apache 2.0 <code>rfc3986-validator</code> Pure python rfc3986 validator <code>&gt;0.1.0</code> <code>0.1.1</code> MIT license <code>rich</code> Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal <code>&gt;=12.0.0</code> <code>13.3.2</code> MIT <code>ruamel-yaml</code> ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order <code>&gt;=0.17.21</code> <code>0.17.21</code> MIT license <code>ruamel-yaml-clib</code> C version of reader, parser and emitter for ruamel.yaml derived from libyaml <code>&gt;=0.2.6; platform_python_implementation == \"CPython\" and python_version &lt; \"3.11\"</code> <code>0.2.7</code> ? <code>ruff</code> An extremely fast Python linter, written in Rust. <code>&gt;=0.0.254</code> <code>0.0.254</code> MIT <code>safety</code> Checks installed dependencies for known vulnerabilities and licenses. <code>&gt;=2</code> <code>2.3.4</code> MIT license <code>secretstorage</code> Python bindings to FreeDesktop.org Secret Service API <code>&gt;=3.2; sys_platform == \"linux\"</code> <code>3.3.3</code> ? <code>semver</code> Python helper for Semantic Versioning (http://semver.org/) <code>~=2.13</code> <code>2.13.0</code> BSD <code>send2trash</code> Send file to trash natively under Mac OS X, Windows and Linux. <code>&gt;=1.8.0</code> <code>1.8.0</code> BSD License <code>setuptools</code> Easily download, build, install, upgrade, and uninstall Python packages <code>&gt;=65.6.3</code> <code>67.6.0</code> MIT License <code>six</code> Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.16.0</code> MIT <code>smmap</code> A pure Python implementation of a sliding window memory map manager <code>&lt;6,&gt;=3.0.1</code> <code>5.0.0</code> BSD <code>sniffio</code> Sniff out which async library your code is running under <code>&gt;=1.1</code> <code>1.3.0</code> MIT OR Apache-2.0 <code>snowballstemmer</code> This package provides 29 stemmers for 28 languages generated from Snowball algorithms. <code>2.2.0</code> BSD-3-Clause <code>soupsieve</code> A modern CSS selector implementation for Beautiful Soup. <code>&gt;1.2</code> <code>2.3.2.post1</code> MIT License <code>stack-data</code> Extract data from python stack frames and tracebacks for informative displays <code>0.6.2</code> MIT <code>stevedore</code> Manage dynamic plugins for Python applications <code>&gt;=1.20.0</code> <code>3.5.2</code> Apache Software License <code>terminado</code> Tornado websocket backend for the Xterm.js Javascript terminal emulator library. <code>&gt;=0.8.3</code> <code>0.17.1</code> BSD License <code>tinycss2</code> A tiny CSS parser <code>1.2.1</code> BSD License <code>toml</code> Python Library for Tom's Obvious, Minimal Language <code>&gt;=0.10</code> <code>0.10.2</code> MIT <code>tomli</code> A lil' TOML parser <code>&gt;=2.0.1; python_version &lt; \"3.11\"</code> <code>2.0.1</code> ? <code>tornado</code> Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed. <code>&gt;=6.1</code> <code>6.2</code> http://www.apache.org/licenses/LICENSE-2.0 <code>traitlets</code> Traitlets Python configuration system <code>&gt;=5.4.0</code> <code>5.8.0</code> BSD License <code>twine</code> Collection of utilities for publishing packages on PyPI <code>&gt;=4.0.2</code> <code>4.0.2</code> Apache Software License <code>types-markdown</code> Typing stubs for Markdown <code>&gt;=3.3</code> <code>3.4.2.5</code> Apache-2.0 license <code>types-python-dateutil</code> Typing stubs for python-dateutil <code>&gt;=2.8.19.5</code> <code>2.8.19.10</code> Apache-2.0 license <code>types-setuptools</code> Typing stubs for setuptools <code>&gt;=65.6.0.2</code> <code>67.6.0.0</code> Apache-2.0 license <code>types-toml</code> Typing stubs for toml <code>&gt;=0.10</code> <code>0.10.8.5</code> Apache-2.0 license <code>typing-extensions</code> Backported and Experimental Type Hints for Python 3.7+ <code>&gt;=3.10.0.0; python_version &lt; \"3.10\"</code> <code>4.4.0</code> Python Software Foundation License <code>uri-template</code> RFC 6570 URI Template Processor <code>1.2.0</code> MIT License <code>urllib3</code> HTTP library with thread-safe connection pooling, file post, and more. <code>&lt;1.27,&gt;=1.21.1</code> <code>1.26.13</code> MIT <code>watchdog</code> Filesystem events monitoring <code>&gt;=2.0</code> <code>2.2.0</code> Apache License 2.0 <code>wcwidth</code> Measures the displayed width of unicode strings in a terminal <code>0.2.5</code> MIT <code>webcolors</code> A library for working with color names and color values formats defined by HTML and CSS. <code>&gt;=1.11</code> <code>1.12</code> BSD 3-Clause <code>webencodings</code> Character encoding aliases for legacy web content <code>&gt;=0.4</code> <code>0.5.1</code> BSD <code>websocket-client</code> WebSocket client for Python with low level API options <code>1.4.2</code> Apache-2.0 <code>widgetsnbextension</code> Jupyter interactive widgets for Jupyter Notebook <code>~=4.0</code> <code>4.0.5</code> BSD 3-Clause License <code>wps-light</code> The strictest and most opinionated python linter ever (lighter fork). <code>&gt;=0.15</code> <code>0.16.1</code> MIT <code>zipp</code> Backport of pathlib-compatible object wrapper for zip files <code>&gt;=0.5</code> <code>3.11.0</code> MIT License"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2021 Bhav Sardana\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"reference/pyef/","title":"pyef","text":"<p>pyef package.</p> <p>Energy Forecasting Toolkit in Python</p>"},{"location":"reference/pyef/#pyef.EnergyTimeFrame","title":"<code>EnergyTimeFrame</code>","text":"<p>Class to preprocess load and temperature data to make it suitable for energy related time series forecasting. It will support electric load forecasting, electricity price forecasting, wind power forecasting and solar power forecasting.</p> Source code in <code>pyef/timeframes.py</code> <pre><code>class EnergyTimeFrame:\n\n\"\"\"\n    Class to preprocess load and temperature data to make it suitable\n    for energy related time series forecasting.\n    It will support electric load forecasting, electricity price forecasting,\n    wind power forecasting and solar power forecasting.\n    \"\"\"\n\n    def __init__(\n        self,\n        kwh_series: pd.Series | pd.DataFrame,\n        weather_series: pd.Series | pd.DataFrame,\n        ex_ante: bool = False,\n        sub_hourly: bool = False,\n    ) -&gt; None:\n\"\"\"TODO add description\n\n        Args:\n            kwh_series (pd.Series | pd.DataFrame): _description_\n            weather_series (pd.Series | pd.DataFrame): _description_\n            ex_ante (bool, optional): _description_. Defaults to False.\n            sub_hourly (bool, optional): _description_. Defaults to False.\n        \"\"\"\n\n        self._original_series = {\n            \"kwh_series\": kwh_series.copy(deep=True),\n            \"weather_series\": weather_series.copy(deep=True),\n        }\n        self.sub_hourly = sub_hourly\n        self._validate()\n        self._freq_warn = False\n        if self.validated:\n            self._kwh_series = kwh_series.copy(deep=True)\n            self._weather_series = weather_series.copy(deep=True)\n\n            self._infer_freq()\n            self._clean_data()\n\n            # add weather features\n            # TODO Move away from adding new features.\n            # Instead, use patsy to create these features\n\n            self._add_dd()\n            self._weather_series = self._add_polynomial(\n                self._weather_series, get_option(\"preprocessing.weather.pol_dict\")\n            )\n            self._weather_series = self._add_lags(\n                self._weather_series, get_option(\"preprocessing.weather.lags_dict\")\n            )\n            self._weather_series = self._add_mas(\n                self._weather_series, get_option(\"preprocessing.weather.mas_dict\")\n            )\n\n            # create combined dataset\n            self._create_feature_dataset()\n\n            # add calendar\n            self._add_calendar()\n        self.cleanup()\n\n    @property\n    def original_series(self) -&gt; dict[str, pd.DataFrame]:\n        return self._original_series\n\n    @property\n    def validated(self) -&gt; bool:\n        return self._validated\n\n    def _validate(self) -&gt; None:\n        if self._validate_series(\n            self._original_series[\"kwh_series\"], \"kwh\"\n        ) &amp; self._validate_series(self._original_series[\"weather_series\"], \"weather\"):\n            self._validated = True\n        else:\n            self._validated = False\n\n    def _validate_series(self, data: pd.DataFrame, series: str) -&gt; bool:\n        # Right now - this takes weather and kwh series with a valid DatetimeIndex\n        # Check the column names\n        cols = list(\n            data.filter(\n                regex=\"|\".join(get_option(f\"preprocessing.{series}.accepted_columns\"))\n            ).columns\n        )\n\n        if data.index.inferred_type != \"datetime64\":\n            msg = f\"Could not validate {series} series. \\\n                Please make sure it has a valid DatetimeIndex\"\n            logger.error(msg)\n            return False\n\n        # if ~ data.index.is_unique:\n        #     msg = f\"Could not validate {series} series. \\\n        #       Please make sure it has unique indices\"\n        #     logger.error(msg)\n        #     return False\n\n        if cols == []:\n            msg = f'Could not validate {series} series.\\\n                Please make sure it includes one of \\\n{get_option(f\"preprocessing.{series}.accepted_columns\")} columns'\n            logger.error(msg)\n            return False\n\n        return True\n\n    def _io_handler(\n        self,\n    ) -&gt; None:\n\"\"\"\n        TODO\n        write  a method to handle the inputs automatically.\n        start with reading csv files.\n        implement reading from databases/aws s3 etc\n        \"\"\"\n\n    def _get_freq(self, df: pd.DataFrame) -&gt; int:\n        freqs, counts = np.unique(\n            np.array((df.index[1:] - df.index[:-1]).total_seconds() / 60).astype(int),\n            return_counts=True,\n        )\n        if freqs.size == 1:\n            # check if infered is equal to freq infered from pandas\n            pd_freq = int(\n                pd.to_timedelta(to_offset(pd.infer_freq(df.index))).total_seconds() / 60\n            )\n            if pd_freq == freqs[0]:\n                return int(freqs[0])\n            else:\n                # still return the infered freq, but set warn to true so\n                # that new index is created in cleaning\n                logger.warning(\"Infered freq did not match pandas' infered freq.\")\n                self._freq_warn = True\n                return int(freqs[0])\n        elif freqs.size &gt; 1:\n            self._freq_warn = True\n            logger.warning(\n                \"Multiple frequencies found. \\\n                This might be because of some missing timestamps in the series index.\"\n            )\n            logger.warning(\n                f\"Frequencies found: {freqs}\\nTheir respective counts: {counts}\"\n            )\n            logger.warning(\n                f\"Using freq: {freqs[np.argmax(counts)]} \\\n                which has most counts {counts[np.argmax(counts)]}\"\n            )\n            return int(freqs[np.argmax(counts)])\n        else:\n            logger.error(f\"{freqs}, {counts}\")\n            raise\n        # return int((df.index[1] - df.index[0]).total_seconds() / 60)\n\n    def _infer_freq(self) -&gt; None:\n\"\"\"\n        TODO: get multiple sample frequencies and warn if not same\n        \"\"\"\n        self._freq_kwh = self._get_freq(self._kwh_series)\n        self._freq_weather = self._get_freq(self._weather_series)\n\n    @property\n    def freq_kwh(self) -&gt; int:\n        return self._freq_kwh\n\n    @property\n    def freq_weather(self) -&gt; int:\n        return self._freq_weather\n\n    def _clean_data(self) -&gt; None:\n        # TODO Add a clean timestamp\n        # self._processing_data = pd.date_range(data.raw_data.index.min(),\n        #  data.raw_data.index.max(), freq='15T')\n        if self._freq_warn:\n            logger.warning(\n                \"Recreating the datetime index for both weather and kwh series\"\n            )\n            # add new datetime index based on infered freq\n            kwh_index = pd.date_range(\n                self._kwh_series.index[0],\n                end=self._kwh_series.index[-1],\n                freq=f\"{self.freq_kwh}min\",\n            )\n            weather_index = pd.date_range(\n                self._weather_series.index[0],\n                end=self._weather_series.index[-1],\n                freq=f\"{self.freq_weather}min\",\n            )\n            self._kwh_series = pd.DataFrame(index=kwh_index).merge(\n                self._kwh_series, left_index=True, right_index=True, how=\"left\"\n            )\n            self._kwh_series.index.name = \"datetime\"\n            self._weather_series = pd.DataFrame(index=weather_index).merge(\n                self._weather_series, left_index=True, right_index=True, how=\"left\"\n            )\n            self._weather_series.index.name = \"datetime\"\n\n        self._kwh_series = self._kwh_series.sort_index().interpolate(\n            method=get_option(\"preprocessing.kwh.fill_na\"),\n            limit_direction=\"forward\",\n            axis=0,\n        )\n        self._weather_series = self._weather_series.sort_index().interpolate(\n            method=get_option(\"preprocessing.weather.fill_na\"),\n            limit_direction=\"forward\",\n            axis=0,\n        )\n\n        if self._weather_series.shape[0] != self._kwh_series.shape[0]:\n            # logger.warn(\"KWH and Weather series are not of the same length.\n            # Making them equal\")\n            if self._weather_series.index.max() &gt; self._kwh_series.index.max():\n                self._weather_series = self._weather_series.loc[\n                    self._weather_series.index &lt; self._kwh_series.index.max()\n                ]\n            else:\n                self._kwh_series = self._kwh_series.loc[\n                    self._kwh_series.index &lt; self._weather_series.index.max()\n                ]\n\n    def _create_feature_dataset(self) -&gt; None:\n        self.feature_dataset = self._kwh_series.merge(\n            self._weather_series,\n            how=\"left\",\n            left_on=self._kwh_series.index,\n            right_on=self._weather_series.index,\n        )\n        self.feature_dataset = self.feature_dataset.rename(\n            columns={\"key_0\": \"datetime\"}\n        ).fillna(method=\"bfill\")\n        self.feature_dataset.index = pd.to_datetime(self.feature_dataset[\"datetime\"])\n        # self.feature_dataset = self.feature_dataset.fillna('bfill')\n\n    @property\n    def temperature_col(self) -&gt; str | None:\n        self._temperature_col = self.infered_series_col(series=\"weather\")\n        return self._temperature_col\n\n    @temperature_col.setter\n    def temperature_col(self, value: str) -&gt; None:\n        self._temperature_col = value\n\n    def infered_series_col(self, series: str) -&gt; str | None:\n        # TODO update this to dynamically read the series and get temperature columns\n        accepted_cols = get_option(f\"preprocessing.{series}.accepted_columns\")\n        series_cols = self.original_series[f\"{series}_series\"].columns\n        infered_series_cols: list[str] = list(\n            set(series_cols).intersection(set(accepted_cols))\n        )\n        if len(infered_series_cols) == 1:\n            return infered_series_cols[0]\n        else:\n            logger.warning(f\"Could not infer {series} column. Please set this manually\")\n            return None\n\n    @property\n    def target_col(self) -&gt; str | None:\n        self._target_col = self.infered_series_col(series=\"kwh\")\n        return self._target_col\n\n    @target_col.setter\n    def target_col(self, value: str) -&gt; None:\n        self._target_col = value\n\n    def _add_lags(\n        self,\n        df: pd.DataFrame,\n        lags_dict: dict[str, int | list[int]],\n    ) -&gt; pd.DataFrame:\n        new_cols = {}\n        # TODO update frequency\n        # TODO update insert logic or get rid? for creating these in forecaster\n        for col, lags in lags_dict.items():\n            if isinstance(lags, int):\n                new_cols[f\"{col}_lag_{lags}\"] = df.loc[:, f\"{col}\"].shift(\n                    lags * int(60 / self._get_freq(df))\n                )\n            else:\n                for lag in lags:\n                    new_cols[f\"{col}_lag_{lag}\"] = df.loc[:, f\"{col}\"].shift(\n                        lag * int(60 / self._get_freq(df))\n                    )\n\n        return pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n\n    def _add_mas(\n        self,\n        df: pd.DataFrame,\n        mas_dict: dict[str, int | list[int]],\n    ) -&gt; pd.DataFrame:\n        new_cols = {}\n        # TODO update frequency\n        # mas in days\n        pd.DataFrame()\n        for col, mas in mas_dict.items():\n            if isinstance(mas, int):\n                new_cols[f\"{col}_ma_{mas}\"] = (\n                    df.loc[:, f\"{col}\"]\n                    .rolling(24 * mas * int(60 / self._get_freq(df)))\n                    .mean()\n                )\n            else:\n                for ma in mas:\n                    new_cols[f\"{col}_ma_{ma}\"] = (\n                        df.loc[:, f\"{col}\"]\n                        .rolling(24 * ma * int(60 / self._get_freq(df)))\n                        .mean()\n                    )\n\n        return pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n\n    def _add_polynomial(\n        self, df: pd.DataFrame, pol_dict: dict[str, int | list[int]]\n    ) -&gt; pd.DataFrame:\n        new_cols = {}\n        for col, pol in pol_dict.items():\n            if isinstance(pol, int):\n                new_cols[f\"{col}_{pol}\"] = df.loc[:, f\"{col}\"].pow(pol)\n            else:\n                for p in pol:\n                    new_cols[f\"{col}_{p}\"] = df.loc[:, f\"{col}\"].pow(p)\n\n        return pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n\n    def _add_dd(self, method: str = \"niave\") -&gt; None:\n        # self._weather_series = pd.concat([self._weather_series, \\\n        # pd.DataFrame(self._weather_series.index, columns=['datetime'])])\n        self._weather_series.insert(\n            loc=0, column=\"datetime\", value=self._weather_series.index\n        )\n        if method == \"niave\":\n            daily_max_temp = pd.DataFrame(\n                self._weather_series.groupby(self._weather_series.index.date)[\n                    f\"{self.temperature_col}\"\n                ].max()\n            )\n            daily_min_temp = pd.DataFrame(\n                self._weather_series.groupby(self._weather_series.index.date)[\n                    f\"{self.temperature_col}\"\n                ].min()\n            )\n            daily_avg_temp = ((daily_max_temp + daily_min_temp) / 2).reset_index()\n            daily_avg_temp.columns = [\"datetime\", \"avg_temperature\"]\n            df_merged = self._weather_series.merge(\n                daily_avg_temp[\"avg_temperature\"],\n                left_on=self._weather_series.index.date,\n                right_on=daily_avg_temp[\"datetime\"],\n            )\n        elif method == \"daily_avg\":\n            daily_avg_temp = pd.DataFrame(\n                self._weather_series.groupby(self._weather_series.index.date)[\n                    f\"{self.temperature_col}\"\n                ].mean()\n            ).reset_index()\n            daily_avg_temp.columns = [\"datetime\", \"avg_temperature\"]\n            df_merged = self._weather_series.merge(\n                daily_avg_temp[\"avg_temperature\"],\n                left_on=self._weather_series.index.date,\n                right_on=daily_avg_temp[\"datetime\"],\n            )\n        else:\n            raise NotImplementedError\n\n        hdd_ref_remp = get_option(\"preprocessing.weather.hdd_ref\")\n        cdd_ref_remp = get_option(\"preprocessing.weather.cdd_ref\")\n\n        df_merged.index = df_merged[\"datetime\"]\n        df_merged.loc[hdd_ref_remp - df_merged[\"avg_temperature\"] &gt;= 0, \"hdd\"] = (\n            hdd_ref_remp\n            - df_merged.loc[\n                hdd_ref_remp - df_merged[\"avg_temperature\"] &gt; 0, \"avg_temperature\"\n            ]\n        )\n        df_merged.loc[hdd_ref_remp - df_merged[\"avg_temperature\"] &lt; 0, \"hdd\"] = 0\n\n        df_merged.loc[df_merged[\"avg_temperature\"] - cdd_ref_remp &gt;= 0, \"cdd\"] = (\n            cdd_ref_remp\n            - df_merged.loc[\n                df_merged[\"avg_temperature\"] - cdd_ref_remp &gt; 0, \"avg_temperature\"\n            ]\n        )\n        df_merged.loc[df_merged[\"avg_temperature\"] - cdd_ref_remp &lt; 0, \"cdd\"] = 0\n\n        self._weather_series = df_merged.drop([\"key_0\", \"datetime\"], axis=1)\n\n    def _add_calendar(self) -&gt; None:\n        self.feature_dataset[\"trend\"] = range(self.feature_dataset.shape[0])\n        self.feature_dataset[\"year\"] = self.feature_dataset.index.year\n        self.feature_dataset[\"month\"] = self.feature_dataset.index.month\n        self.feature_dataset[\"day_of_week\"] = self.feature_dataset.index.day_of_week\n        self.feature_dataset[\"hour\"] = self.feature_dataset.index.hour\n        if self.sub_hourly:\n            self.feature_dataset[\"minute\"] = self.feature_dataset.index.minute\n\n    def cleanup(self) -&gt; None:\n        ...\n\n    def plot(self, **kwargs: Any) -&gt; pd.plotting.PlotAccessor:\n        fig = self.feature_dataset.plot(**kwargs)\n        return fig\n</code></pre>"},{"location":"reference/pyef/#pyef.timeframes.EnergyTimeFrame.__init__","title":"<code>__init__(kwh_series, weather_series, ex_ante=False, sub_hourly=False)</code>","text":"<p>TODO add description</p> <p>Parameters:</p> Name Type Description Default <code>kwh_series</code> <code>pd.Series | pd.DataFrame</code> <p>description</p> required <code>weather_series</code> <code>pd.Series | pd.DataFrame</code> <p>description</p> required <code>ex_ante</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>sub_hourly</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> Source code in <code>pyef/timeframes.py</code> <pre><code>def __init__(\n    self,\n    kwh_series: pd.Series | pd.DataFrame,\n    weather_series: pd.Series | pd.DataFrame,\n    ex_ante: bool = False,\n    sub_hourly: bool = False,\n) -&gt; None:\n\"\"\"TODO add description\n\n    Args:\n        kwh_series (pd.Series | pd.DataFrame): _description_\n        weather_series (pd.Series | pd.DataFrame): _description_\n        ex_ante (bool, optional): _description_. Defaults to False.\n        sub_hourly (bool, optional): _description_. Defaults to False.\n    \"\"\"\n\n    self._original_series = {\n        \"kwh_series\": kwh_series.copy(deep=True),\n        \"weather_series\": weather_series.copy(deep=True),\n    }\n    self.sub_hourly = sub_hourly\n    self._validate()\n    self._freq_warn = False\n    if self.validated:\n        self._kwh_series = kwh_series.copy(deep=True)\n        self._weather_series = weather_series.copy(deep=True)\n\n        self._infer_freq()\n        self._clean_data()\n\n        # add weather features\n        # TODO Move away from adding new features.\n        # Instead, use patsy to create these features\n\n        self._add_dd()\n        self._weather_series = self._add_polynomial(\n            self._weather_series, get_option(\"preprocessing.weather.pol_dict\")\n        )\n        self._weather_series = self._add_lags(\n            self._weather_series, get_option(\"preprocessing.weather.lags_dict\")\n        )\n        self._weather_series = self._add_mas(\n            self._weather_series, get_option(\"preprocessing.weather.mas_dict\")\n        )\n\n        # create combined dataset\n        self._create_feature_dataset()\n\n        # add calendar\n        self._add_calendar()\n    self.cleanup()\n</code></pre>"},{"location":"reference/pyef/_config/","title":"_config","text":""},{"location":"reference/pyef/cli/","title":"cli","text":"<p>Module that contains the command line application.</p>"},{"location":"reference/pyef/cli/#pyef.cli.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the CLI argument parser.</p> <p>Returns:</p> Type Description <code>argparse.ArgumentParser</code> <p>An argparse parser.</p> Source code in <code>pyef/cli.py</code> <pre><code>def get_parser() -&gt; argparse.ArgumentParser:\n\"\"\"\n    Return the CLI argument parser.\n\n    Returns:\n        An argparse parser.\n    \"\"\"\n    return argparse.ArgumentParser(prog=\"pyef\")\n</code></pre>"},{"location":"reference/pyef/cli/#pyef.cli.main","title":"<code>main(args=None)</code>","text":"<p>Run the main program.</p> <p>This function is executed when you type <code>pyef</code> or <code>python -m pyef</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Optional[List[str]]</code> <p>Arguments passed from the command line.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>An exit code.</p> Source code in <code>pyef/cli.py</code> <pre><code>def main(args: Optional[List[str]] = None) -&gt; int:\n\"\"\"\n    Run the main program.\n\n    This function is executed when you type `pyef` or `python -m pyef`.\n\n    Arguments:\n        args: Arguments passed from the command line.\n\n    Returns:\n        An exit code.\n    \"\"\"\n    parser = get_parser()\n    opts = parser.parse_args(args=args)\n    print(opts)  # noqa: WPS421 (side-effect in main is fine)\n    return 0\n</code></pre>"},{"location":"reference/pyef/combination/","title":"combination","text":"<p>TODO design combination Combination class for generating model combinations</p> <p>Ideas:</p> inputs <ul> <li> <p>multiple forecaster objects with self.predicted = True validate all input models - they should have the same input etf, and same input forecaster parameters like pred_start and horizon</p> </li> <li> <p>method - combination method to be applied, eg. simple average, weighted average, median, min and max pred at each timestep, advanced - stacking. take output of each model to train a new forecaster object and generate predictions</p> </li> </ul>"},{"location":"reference/pyef/datasets/","title":"datasets","text":""},{"location":"reference/pyef/datasets/#pyef.datasets.bigdeal_final_2022","title":"<code>bigdeal_final_2022()</code>","text":"<p>summary</p> <p>Returns:</p> Type Description <code>dict[str, pd.DataFrame]</code> <p>dict[str, pd.DataFrame]: description</p> Source code in <code>pyef/datasets.py</code> <pre><code>def bigdeal_final_2022() -&gt; dict[str, pd.DataFrame]:\n\"\"\"_summary_\n\n    Returns:\n        dict[str, pd.DataFrame]: _description_\n    \"\"\"\n\n    ref = importlib_resources.files(\"pyef\") / os.path.join(\n        \"data\", \"bigdeal2022\", \"final_match\"\n    )\n    with importlib_resources.as_file(ref) as filepath:\n        data = pd.read_csv(f\"{filepath}/final_match.csv.zip\")\n\n    data.columns = data.columns.str.lower()\n    data.index = pd.to_datetime(data[\"date\"])\n    data.index += pd.TimedeltaIndex(data[\"hour\"], unit=\"h\")\n    data.index.name = \"datetime\"\n\n    df_load_1 = data.loc[:, [\"ldc1\"]]\n    df_load_1.columns = [\"load\"]\n    df_load_1[\"zone_id\"] = 1\n\n    df_load_2 = data.loc[:, [\"ldc2\"]]\n    df_load_2.columns = [\"load\"]\n    df_load_2[\"zone_id\"] = 2\n\n    df_load_3 = data.loc[:, [\"ldc3\"]]\n    df_load_3.columns = [\"load\"]\n    df_load_3[\"zone_id\"] = 3\n\n    df_load = pd.concat(\n        [\n            df_load_1,\n            df_load_2,\n            df_load_3,\n        ]\n    )\n\n    df_temperature_1 = data.loc[:, [\"t1\"]]\n    df_temperature_1.columns = [\"temperature\"]\n    df_temperature_1[\"station_id\"] = 1\n\n    df_temperature_2 = data.loc[:, [\"t2\"]]\n    df_temperature_2.columns = [\"temperature\"]\n    df_temperature_2[\"station_id\"] = 2\n\n    df_temperature_3 = data.loc[:, [\"t3\"]]\n    df_temperature_3.columns = [\"temperature\"]\n    df_temperature_3[\"station_id\"] = 3\n\n    df_temperature_4 = data.loc[:, [\"t4\"]]\n    df_temperature_4.columns = [\"temperature\"]\n    df_temperature_4[\"station_id\"] = 4\n\n    df_temperature_5 = data.loc[:, [\"t5\"]]\n    df_temperature_5.columns = [\"temperature\"]\n    df_temperature_5[\"station_id\"] = 5\n\n    df_temperature_6 = data.loc[:, [\"t6\"]]\n    df_temperature_6.columns = [\"temperature\"]\n    df_temperature_6[\"station_id\"] = 6\n\n    df_temperature = pd.concat(\n        [\n            df_temperature_1,\n            df_temperature_2,\n            df_temperature_3,\n            df_temperature_4,\n            df_temperature_5,\n            df_temperature_6,\n        ]\n    )\n\n    df_temperature_forecast_1 = data.loc[:, [\"t1_forecast\"]]\n    df_temperature_forecast_1.columns = [\"temperature_forecast\"]\n    df_temperature_forecast_1[\"station_id\"] = 1\n\n    df_temperature_forecast_2 = data.loc[:, [\"t2_forecast\"]]\n    df_temperature_forecast_2.columns = [\"temperature_forecast\"]\n    df_temperature_forecast_2[\"station_id\"] = 2\n\n    df_temperature_forecast_3 = data.loc[:, [\"t3_forecast\"]]\n    df_temperature_forecast_3.columns = [\"temperature_forecast\"]\n    df_temperature_forecast_3[\"station_id\"] = 3\n\n    df_temperature_forecast_4 = data.loc[:, [\"t4_forecast\"]]\n    df_temperature_forecast_4.columns = [\"temperature_forecast\"]\n    df_temperature_forecast_4[\"station_id\"] = 4\n\n    df_temperature_forecast_5 = data.loc[:, [\"t5_forecast\"]]\n    df_temperature_forecast_5.columns = [\"temperature_forecast\"]\n    df_temperature_forecast_5[\"station_id\"] = 5\n\n    df_temperature_forecast_6 = data.loc[:, [\"t6_forecast\"]]\n    df_temperature_forecast_6.columns = [\"temperature_forecast\"]\n    df_temperature_forecast_6[\"station_id\"] = 6\n\n    df_temperature_forecast = pd.concat(\n        [\n            df_temperature_forecast_1,\n            df_temperature_forecast_2,\n            df_temperature_forecast_3,\n            df_temperature_forecast_4,\n            df_temperature_forecast_5,\n            df_temperature_forecast_6,\n        ]\n    )\n\n    return {\n        \"load\": df_load,\n        \"temperature\": df_temperature,\n        \"temperature_forecast\": df_temperature_forecast,\n    }\n</code></pre>"},{"location":"reference/pyef/datasets/#pyef.datasets.bigdeal_qualifying_2022","title":"<code>bigdeal_qualifying_2022()</code>","text":"<p>summary</p> <p>Returns:</p> Type Description <code>dict[str, pd.DataFrame]</code> <p>dict[str, pd.DataFrame]: description</p> Source code in <code>pyef/datasets.py</code> <pre><code>def bigdeal_qualifying_2022() -&gt; dict[str, pd.DataFrame]:\n\"\"\"_summary_\n\n    Returns:\n        dict[str, pd.DataFrame]: _description_\n    \"\"\"\n\n    ref = importlib_resources.files(\"pyef\") / os.path.join(\n        \"data\", \"bigdeal2022\", \"qualifying_match\"\n    )\n    with importlib_resources.as_file(ref) as filepath:\n        data = pd.read_csv(f\"{filepath}/data_round_1.csv.zip\")\n\n    data.columns = data.columns.str.lower()\n    data[\"datetime\"] = pd.to_datetime(data[[\"year\", \"month\", \"day\", \"hour\"]])\n    data.index = data[\"datetime\"]\n\n    df_load = data.loc[:, [\"load\"]]\n    df_load[\"zone_id\"] = 1\n\n    df_temperature_1 = data.loc[:, [\"t1\"]]\n    df_temperature_1.columns = [\"temperature\"]\n    df_temperature_1[\"station_id\"] = 1\n\n    df_temperature_2 = data.loc[:, [\"t2\"]]\n    df_temperature_2.columns = [\"temperature\"]\n    df_temperature_2[\"station_id\"] = 2\n\n    df_temperature_3 = data.loc[:, [\"t3\"]]\n    df_temperature_3.columns = [\"temperature\"]\n    df_temperature_3[\"station_id\"] = 3\n\n    df_temperature_4 = data.loc[:, [\"t4\"]]\n    df_temperature_4.columns = [\"temperature\"]\n    df_temperature_4[\"station_id\"] = 4\n\n    df_temperature = pd.concat(\n        [df_temperature_1, df_temperature_2, df_temperature_3, df_temperature_4]\n    )\n\n    return {\"load\": df_load, \"temperature\": df_temperature}\n</code></pre>"},{"location":"reference/pyef/datasets/#pyef.datasets.gefcom_load_2012","title":"<code>gefcom_load_2012()</code>","text":"<p>Loads GEFCom 2012 dataset src: https://www.dropbox.com/s/epj9b57eivn79j7/GEFCom2012.zip?dl=1</p> <p>TODO add references TODO add examples</p> Source code in <code>pyef/datasets.py</code> <pre><code>def gefcom_load_2012() -&gt; dict[str, pd.DataFrame]:\n\"\"\"\n    Loads GEFCom 2012 dataset\n    src: https://www.dropbox.com/s/epj9b57eivn79j7/GEFCom2012.zip?dl=1\n\n    TODO add references\n    TODO add examples\n\n    \"\"\"\n    # TODO fix hour - hour ending (only change 24th hour to 0)\n    melt_cols = [\"year\", \"month\", \"day\"]\n    ref = importlib_resources.files(\"pyef\") / os.path.join(\"data\", \"gefcom2012\", \"load\")\n    with importlib_resources.as_file(ref) as filepath:\n        load_history = pd.read_csv(f\"{filepath}/Load_history.csv.zip\", thousands=\",\")\n        load_solution = pd.read_csv(\n            f\"{filepath}/Load_solution.csv.zip\", thousands=\",\"\n        ).drop([\"weight\", \"id\"], axis=1)\n\n    def convert_to_ts(df_load: pd.DataFrame) -&gt; pd.DataFrame:\n        df_load = pd.melt(\n            df_load, id_vars=[\"zone_id\"] + melt_cols, var_name=\"hour\", value_name=\"load\"\n        )\n        df_load[\"hour\"] = df_load[\"hour\"].str.replace(\"h\", \"\")\n        df_load[\"hour\"] = df_load[\"hour\"].apply(lambda x: int(x))\n        df_load[\"datetime\"] = pd.to_datetime(df_load[[\"year\", \"month\", \"day\", \"hour\"]])\n        df_load = df_load.drop([\"year\", \"month\", \"day\", \"hour\"], axis=1)\n        df_load = df_load.set_index(\"datetime\").sort_index()\n        return df_load\n\n    load_history = convert_to_ts(df_load=load_history)\n    load_solution = convert_to_ts(df_load=load_solution)\n\n    df_load = (\n        load_history.set_index(\"zone_id\", append=True)\n        .fillna(load_solution.set_index(\"zone_id\", append=True))\n        .reset_index(level=1)\n    )\n\n    df_temperature_1 = pd.read_csv(f\"{filepath}/temperature_history.csv.zip\")\n\n    df_temperature_1 = pd.melt(\n        df_temperature_1,\n        id_vars=[\"station_id\"] + melt_cols,\n        var_name=\"hour\",\n        value_name=\"temperature\",\n    )\n    df_temperature_1[\"hour\"] = df_temperature_1[\"hour\"].str.replace(\"h\", \"\")\n    df_temperature_1[\"hour\"] = df_temperature_1[\"hour\"].apply(lambda x: int(x))\n    df_temperature_1[\"datetime\"] = pd.to_datetime(\n        df_temperature_1[[\"year\", \"month\", \"day\", \"hour\"]]\n    )\n    df_temperature_1 = df_temperature_1.drop([\"year\", \"month\", \"day\", \"hour\"], axis=1)\n    df_temperature_1 = df_temperature_1.set_index(\"datetime\")\n\n    df_temperature_2 = (\n        pd.read_csv(f\"{filepath}/temperature_solution.csv.zip\")\n        .drop([\"datetime\", \"date\"], axis=1)\n        .rename(columns={\"T0_p1\": \"temperature\"})\n    )\n\n    df_temperature_2[\"datetime\"] = pd.to_datetime(\n        df_temperature_2[[\"year\", \"month\", \"day\", \"hour\"]]\n    )\n    df_temperature_2 = df_temperature_2.drop([\"year\", \"month\", \"day\", \"hour\"], axis=1)\n    df_temperature_2 = df_temperature_2.set_index(\"datetime\")\n\n    # df_temperature = pd.concat([df_temperature_1, df_temperature_2]).sort_index()\n    df_temperature = (\n        df_temperature_1.set_index(\"station_id\", append=True)\n        .fillna(df_temperature_2.set_index(\"station_id\", append=True))\n        .reset_index(level=1)\n    ).sort_index()\n\n    return {\"load\": df_load, \"temperature\": df_temperature}\n</code></pre>"},{"location":"reference/pyef/evaluator/","title":"evaluator","text":"<p>Evaluator</p>"},{"location":"reference/pyef/forecaster/","title":"forecaster","text":"<p>Forecaster class. TODO add shit.</p>"},{"location":"reference/pyef/logger/","title":"logger","text":"<p>logger this module should not exist</p>"},{"location":"reference/pyef/logger/#pyef.logger.get_logger","title":"<code>get_logger(name)</code>","text":"<p>TODO add summary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>description</p> required <p>Returns:</p> Type Description <code>logging.Logger</code> <p>logging.Logger: description</p> Source code in <code>pyef/logger.py</code> <pre><code>def get_logger(\n    name: str,\n) -&gt; logging.Logger:\n\"\"\"TODO add summary.\n\n    Args:\n        name (str): _description_\n\n    Returns:\n        logging.Logger: _description_\n    \"\"\"\n    level = get_option(\"logging_level\")\n    logger = logging.getLogger(name)\n    shell_handler = RichHandler()\n    logger.setLevel(level)\n    shell_handler.setLevel(level)\n    fmt_shell = \"[%(filename)s:%(funcName)s:%(lineno)d] %(message)s\"\n    shell_formatter = logging.Formatter(fmt_shell)\n    shell_handler.setFormatter(shell_formatter)\n    logger.addHandler(shell_handler)\n    return logger\n</code></pre>"},{"location":"reference/pyef/timeframes/","title":"timeframes","text":"<p>Contains base classes - TimeSeries and TimeFrames.</p> <p>These perform preprocessing on input data and converts it to a format which is suitable for feeding to sklearn compatible models</p>"},{"location":"reference/pyef/timeframes/#pyef.timeframes.EnergyTimeFrame","title":"<code>EnergyTimeFrame</code>","text":"<p>Class to preprocess load and temperature data to make it suitable for energy related time series forecasting. It will support electric load forecasting, electricity price forecasting, wind power forecasting and solar power forecasting.</p> Source code in <code>pyef/timeframes.py</code> <pre><code>class EnergyTimeFrame:\n\n\"\"\"\n    Class to preprocess load and temperature data to make it suitable\n    for energy related time series forecasting.\n    It will support electric load forecasting, electricity price forecasting,\n    wind power forecasting and solar power forecasting.\n    \"\"\"\n\n    def __init__(\n        self,\n        kwh_series: pd.Series | pd.DataFrame,\n        weather_series: pd.Series | pd.DataFrame,\n        ex_ante: bool = False,\n        sub_hourly: bool = False,\n    ) -&gt; None:\n\"\"\"TODO add description\n\n        Args:\n            kwh_series (pd.Series | pd.DataFrame): _description_\n            weather_series (pd.Series | pd.DataFrame): _description_\n            ex_ante (bool, optional): _description_. Defaults to False.\n            sub_hourly (bool, optional): _description_. Defaults to False.\n        \"\"\"\n\n        self._original_series = {\n            \"kwh_series\": kwh_series.copy(deep=True),\n            \"weather_series\": weather_series.copy(deep=True),\n        }\n        self.sub_hourly = sub_hourly\n        self._validate()\n        self._freq_warn = False\n        if self.validated:\n            self._kwh_series = kwh_series.copy(deep=True)\n            self._weather_series = weather_series.copy(deep=True)\n\n            self._infer_freq()\n            self._clean_data()\n\n            # add weather features\n            # TODO Move away from adding new features.\n            # Instead, use patsy to create these features\n\n            self._add_dd()\n            self._weather_series = self._add_polynomial(\n                self._weather_series, get_option(\"preprocessing.weather.pol_dict\")\n            )\n            self._weather_series = self._add_lags(\n                self._weather_series, get_option(\"preprocessing.weather.lags_dict\")\n            )\n            self._weather_series = self._add_mas(\n                self._weather_series, get_option(\"preprocessing.weather.mas_dict\")\n            )\n\n            # create combined dataset\n            self._create_feature_dataset()\n\n            # add calendar\n            self._add_calendar()\n        self.cleanup()\n\n    @property\n    def original_series(self) -&gt; dict[str, pd.DataFrame]:\n        return self._original_series\n\n    @property\n    def validated(self) -&gt; bool:\n        return self._validated\n\n    def _validate(self) -&gt; None:\n        if self._validate_series(\n            self._original_series[\"kwh_series\"], \"kwh\"\n        ) &amp; self._validate_series(self._original_series[\"weather_series\"], \"weather\"):\n            self._validated = True\n        else:\n            self._validated = False\n\n    def _validate_series(self, data: pd.DataFrame, series: str) -&gt; bool:\n        # Right now - this takes weather and kwh series with a valid DatetimeIndex\n        # Check the column names\n        cols = list(\n            data.filter(\n                regex=\"|\".join(get_option(f\"preprocessing.{series}.accepted_columns\"))\n            ).columns\n        )\n\n        if data.index.inferred_type != \"datetime64\":\n            msg = f\"Could not validate {series} series. \\\n                Please make sure it has a valid DatetimeIndex\"\n            logger.error(msg)\n            return False\n\n        # if ~ data.index.is_unique:\n        #     msg = f\"Could not validate {series} series. \\\n        #       Please make sure it has unique indices\"\n        #     logger.error(msg)\n        #     return False\n\n        if cols == []:\n            msg = f'Could not validate {series} series.\\\n                Please make sure it includes one of \\\n{get_option(f\"preprocessing.{series}.accepted_columns\")} columns'\n            logger.error(msg)\n            return False\n\n        return True\n\n    def _io_handler(\n        self,\n    ) -&gt; None:\n\"\"\"\n        TODO\n        write  a method to handle the inputs automatically.\n        start with reading csv files.\n        implement reading from databases/aws s3 etc\n        \"\"\"\n\n    def _get_freq(self, df: pd.DataFrame) -&gt; int:\n        freqs, counts = np.unique(\n            np.array((df.index[1:] - df.index[:-1]).total_seconds() / 60).astype(int),\n            return_counts=True,\n        )\n        if freqs.size == 1:\n            # check if infered is equal to freq infered from pandas\n            pd_freq = int(\n                pd.to_timedelta(to_offset(pd.infer_freq(df.index))).total_seconds() / 60\n            )\n            if pd_freq == freqs[0]:\n                return int(freqs[0])\n            else:\n                # still return the infered freq, but set warn to true so\n                # that new index is created in cleaning\n                logger.warning(\"Infered freq did not match pandas' infered freq.\")\n                self._freq_warn = True\n                return int(freqs[0])\n        elif freqs.size &gt; 1:\n            self._freq_warn = True\n            logger.warning(\n                \"Multiple frequencies found. \\\n                This might be because of some missing timestamps in the series index.\"\n            )\n            logger.warning(\n                f\"Frequencies found: {freqs}\\nTheir respective counts: {counts}\"\n            )\n            logger.warning(\n                f\"Using freq: {freqs[np.argmax(counts)]} \\\n                which has most counts {counts[np.argmax(counts)]}\"\n            )\n            return int(freqs[np.argmax(counts)])\n        else:\n            logger.error(f\"{freqs}, {counts}\")\n            raise\n        # return int((df.index[1] - df.index[0]).total_seconds() / 60)\n\n    def _infer_freq(self) -&gt; None:\n\"\"\"\n        TODO: get multiple sample frequencies and warn if not same\n        \"\"\"\n        self._freq_kwh = self._get_freq(self._kwh_series)\n        self._freq_weather = self._get_freq(self._weather_series)\n\n    @property\n    def freq_kwh(self) -&gt; int:\n        return self._freq_kwh\n\n    @property\n    def freq_weather(self) -&gt; int:\n        return self._freq_weather\n\n    def _clean_data(self) -&gt; None:\n        # TODO Add a clean timestamp\n        # self._processing_data = pd.date_range(data.raw_data.index.min(),\n        #  data.raw_data.index.max(), freq='15T')\n        if self._freq_warn:\n            logger.warning(\n                \"Recreating the datetime index for both weather and kwh series\"\n            )\n            # add new datetime index based on infered freq\n            kwh_index = pd.date_range(\n                self._kwh_series.index[0],\n                end=self._kwh_series.index[-1],\n                freq=f\"{self.freq_kwh}min\",\n            )\n            weather_index = pd.date_range(\n                self._weather_series.index[0],\n                end=self._weather_series.index[-1],\n                freq=f\"{self.freq_weather}min\",\n            )\n            self._kwh_series = pd.DataFrame(index=kwh_index).merge(\n                self._kwh_series, left_index=True, right_index=True, how=\"left\"\n            )\n            self._kwh_series.index.name = \"datetime\"\n            self._weather_series = pd.DataFrame(index=weather_index).merge(\n                self._weather_series, left_index=True, right_index=True, how=\"left\"\n            )\n            self._weather_series.index.name = \"datetime\"\n\n        self._kwh_series = self._kwh_series.sort_index().interpolate(\n            method=get_option(\"preprocessing.kwh.fill_na\"),\n            limit_direction=\"forward\",\n            axis=0,\n        )\n        self._weather_series = self._weather_series.sort_index().interpolate(\n            method=get_option(\"preprocessing.weather.fill_na\"),\n            limit_direction=\"forward\",\n            axis=0,\n        )\n\n        if self._weather_series.shape[0] != self._kwh_series.shape[0]:\n            # logger.warn(\"KWH and Weather series are not of the same length.\n            # Making them equal\")\n            if self._weather_series.index.max() &gt; self._kwh_series.index.max():\n                self._weather_series = self._weather_series.loc[\n                    self._weather_series.index &lt; self._kwh_series.index.max()\n                ]\n            else:\n                self._kwh_series = self._kwh_series.loc[\n                    self._kwh_series.index &lt; self._weather_series.index.max()\n                ]\n\n    def _create_feature_dataset(self) -&gt; None:\n        self.feature_dataset = self._kwh_series.merge(\n            self._weather_series,\n            how=\"left\",\n            left_on=self._kwh_series.index,\n            right_on=self._weather_series.index,\n        )\n        self.feature_dataset = self.feature_dataset.rename(\n            columns={\"key_0\": \"datetime\"}\n        ).fillna(method=\"bfill\")\n        self.feature_dataset.index = pd.to_datetime(self.feature_dataset[\"datetime\"])\n        # self.feature_dataset = self.feature_dataset.fillna('bfill')\n\n    @property\n    def temperature_col(self) -&gt; str | None:\n        self._temperature_col = self.infered_series_col(series=\"weather\")\n        return self._temperature_col\n\n    @temperature_col.setter\n    def temperature_col(self, value: str) -&gt; None:\n        self._temperature_col = value\n\n    def infered_series_col(self, series: str) -&gt; str | None:\n        # TODO update this to dynamically read the series and get temperature columns\n        accepted_cols = get_option(f\"preprocessing.{series}.accepted_columns\")\n        series_cols = self.original_series[f\"{series}_series\"].columns\n        infered_series_cols: list[str] = list(\n            set(series_cols).intersection(set(accepted_cols))\n        )\n        if len(infered_series_cols) == 1:\n            return infered_series_cols[0]\n        else:\n            logger.warning(f\"Could not infer {series} column. Please set this manually\")\n            return None\n\n    @property\n    def target_col(self) -&gt; str | None:\n        self._target_col = self.infered_series_col(series=\"kwh\")\n        return self._target_col\n\n    @target_col.setter\n    def target_col(self, value: str) -&gt; None:\n        self._target_col = value\n\n    def _add_lags(\n        self,\n        df: pd.DataFrame,\n        lags_dict: dict[str, int | list[int]],\n    ) -&gt; pd.DataFrame:\n        new_cols = {}\n        # TODO update frequency\n        # TODO update insert logic or get rid? for creating these in forecaster\n        for col, lags in lags_dict.items():\n            if isinstance(lags, int):\n                new_cols[f\"{col}_lag_{lags}\"] = df.loc[:, f\"{col}\"].shift(\n                    lags * int(60 / self._get_freq(df))\n                )\n            else:\n                for lag in lags:\n                    new_cols[f\"{col}_lag_{lag}\"] = df.loc[:, f\"{col}\"].shift(\n                        lag * int(60 / self._get_freq(df))\n                    )\n\n        return pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n\n    def _add_mas(\n        self,\n        df: pd.DataFrame,\n        mas_dict: dict[str, int | list[int]],\n    ) -&gt; pd.DataFrame:\n        new_cols = {}\n        # TODO update frequency\n        # mas in days\n        pd.DataFrame()\n        for col, mas in mas_dict.items():\n            if isinstance(mas, int):\n                new_cols[f\"{col}_ma_{mas}\"] = (\n                    df.loc[:, f\"{col}\"]\n                    .rolling(24 * mas * int(60 / self._get_freq(df)))\n                    .mean()\n                )\n            else:\n                for ma in mas:\n                    new_cols[f\"{col}_ma_{ma}\"] = (\n                        df.loc[:, f\"{col}\"]\n                        .rolling(24 * ma * int(60 / self._get_freq(df)))\n                        .mean()\n                    )\n\n        return pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n\n    def _add_polynomial(\n        self, df: pd.DataFrame, pol_dict: dict[str, int | list[int]]\n    ) -&gt; pd.DataFrame:\n        new_cols = {}\n        for col, pol in pol_dict.items():\n            if isinstance(pol, int):\n                new_cols[f\"{col}_{pol}\"] = df.loc[:, f\"{col}\"].pow(pol)\n            else:\n                for p in pol:\n                    new_cols[f\"{col}_{p}\"] = df.loc[:, f\"{col}\"].pow(p)\n\n        return pd.concat([df, pd.DataFrame(new_cols)], axis=1)\n\n    def _add_dd(self, method: str = \"niave\") -&gt; None:\n        # self._weather_series = pd.concat([self._weather_series, \\\n        # pd.DataFrame(self._weather_series.index, columns=['datetime'])])\n        self._weather_series.insert(\n            loc=0, column=\"datetime\", value=self._weather_series.index\n        )\n        if method == \"niave\":\n            daily_max_temp = pd.DataFrame(\n                self._weather_series.groupby(self._weather_series.index.date)[\n                    f\"{self.temperature_col}\"\n                ].max()\n            )\n            daily_min_temp = pd.DataFrame(\n                self._weather_series.groupby(self._weather_series.index.date)[\n                    f\"{self.temperature_col}\"\n                ].min()\n            )\n            daily_avg_temp = ((daily_max_temp + daily_min_temp) / 2).reset_index()\n            daily_avg_temp.columns = [\"datetime\", \"avg_temperature\"]\n            df_merged = self._weather_series.merge(\n                daily_avg_temp[\"avg_temperature\"],\n                left_on=self._weather_series.index.date,\n                right_on=daily_avg_temp[\"datetime\"],\n            )\n        elif method == \"daily_avg\":\n            daily_avg_temp = pd.DataFrame(\n                self._weather_series.groupby(self._weather_series.index.date)[\n                    f\"{self.temperature_col}\"\n                ].mean()\n            ).reset_index()\n            daily_avg_temp.columns = [\"datetime\", \"avg_temperature\"]\n            df_merged = self._weather_series.merge(\n                daily_avg_temp[\"avg_temperature\"],\n                left_on=self._weather_series.index.date,\n                right_on=daily_avg_temp[\"datetime\"],\n            )\n        else:\n            raise NotImplementedError\n\n        hdd_ref_remp = get_option(\"preprocessing.weather.hdd_ref\")\n        cdd_ref_remp = get_option(\"preprocessing.weather.cdd_ref\")\n\n        df_merged.index = df_merged[\"datetime\"]\n        df_merged.loc[hdd_ref_remp - df_merged[\"avg_temperature\"] &gt;= 0, \"hdd\"] = (\n            hdd_ref_remp\n            - df_merged.loc[\n                hdd_ref_remp - df_merged[\"avg_temperature\"] &gt; 0, \"avg_temperature\"\n            ]\n        )\n        df_merged.loc[hdd_ref_remp - df_merged[\"avg_temperature\"] &lt; 0, \"hdd\"] = 0\n\n        df_merged.loc[df_merged[\"avg_temperature\"] - cdd_ref_remp &gt;= 0, \"cdd\"] = (\n            cdd_ref_remp\n            - df_merged.loc[\n                df_merged[\"avg_temperature\"] - cdd_ref_remp &gt; 0, \"avg_temperature\"\n            ]\n        )\n        df_merged.loc[df_merged[\"avg_temperature\"] - cdd_ref_remp &lt; 0, \"cdd\"] = 0\n\n        self._weather_series = df_merged.drop([\"key_0\", \"datetime\"], axis=1)\n\n    def _add_calendar(self) -&gt; None:\n        self.feature_dataset[\"trend\"] = range(self.feature_dataset.shape[0])\n        self.feature_dataset[\"year\"] = self.feature_dataset.index.year\n        self.feature_dataset[\"month\"] = self.feature_dataset.index.month\n        self.feature_dataset[\"day_of_week\"] = self.feature_dataset.index.day_of_week\n        self.feature_dataset[\"hour\"] = self.feature_dataset.index.hour\n        if self.sub_hourly:\n            self.feature_dataset[\"minute\"] = self.feature_dataset.index.minute\n\n    def cleanup(self) -&gt; None:\n        ...\n\n    def plot(self, **kwargs: Any) -&gt; pd.plotting.PlotAccessor:\n        fig = self.feature_dataset.plot(**kwargs)\n        return fig\n</code></pre>"},{"location":"reference/pyef/timeframes/#pyef.timeframes.EnergyTimeFrame.__init__","title":"<code>__init__(kwh_series, weather_series, ex_ante=False, sub_hourly=False)</code>","text":"<p>TODO add description</p> <p>Parameters:</p> Name Type Description Default <code>kwh_series</code> <code>pd.Series | pd.DataFrame</code> <p>description</p> required <code>weather_series</code> <code>pd.Series | pd.DataFrame</code> <p>description</p> required <code>ex_ante</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>sub_hourly</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> Source code in <code>pyef/timeframes.py</code> <pre><code>def __init__(\n    self,\n    kwh_series: pd.Series | pd.DataFrame,\n    weather_series: pd.Series | pd.DataFrame,\n    ex_ante: bool = False,\n    sub_hourly: bool = False,\n) -&gt; None:\n\"\"\"TODO add description\n\n    Args:\n        kwh_series (pd.Series | pd.DataFrame): _description_\n        weather_series (pd.Series | pd.DataFrame): _description_\n        ex_ante (bool, optional): _description_. Defaults to False.\n        sub_hourly (bool, optional): _description_. Defaults to False.\n    \"\"\"\n\n    self._original_series = {\n        \"kwh_series\": kwh_series.copy(deep=True),\n        \"weather_series\": weather_series.copy(deep=True),\n    }\n    self.sub_hourly = sub_hourly\n    self._validate()\n    self._freq_warn = False\n    if self.validated:\n        self._kwh_series = kwh_series.copy(deep=True)\n        self._weather_series = weather_series.copy(deep=True)\n\n        self._infer_freq()\n        self._clean_data()\n\n        # add weather features\n        # TODO Move away from adding new features.\n        # Instead, use patsy to create these features\n\n        self._add_dd()\n        self._weather_series = self._add_polynomial(\n            self._weather_series, get_option(\"preprocessing.weather.pol_dict\")\n        )\n        self._weather_series = self._add_lags(\n            self._weather_series, get_option(\"preprocessing.weather.lags_dict\")\n        )\n        self._weather_series = self._add_mas(\n            self._weather_series, get_option(\"preprocessing.weather.mas_dict\")\n        )\n\n        # create combined dataset\n        self._create_feature_dataset()\n\n        # add calendar\n        self._add_calendar()\n    self.cleanup()\n</code></pre>"},{"location":"coverage/","title":"Coverage report","text":""}]}